[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Remote Sensing Learning Diary",
    "section": "",
    "text": "Last updated: 2023-01-27\n\nPreface\nThis is a Learning Diary for MSc Urban Spatial Science CASA0023 Remotely Sensing Cities and Environment module.\nThis website contains weekly learning material with structures below:\n\nSummary\nApplications\nReflection\nConclusion\n\n\n\nAbout me\nMy name is Rahmadita Listianingrum. I am an international student from Indonesia studying at MSc Urban Spatial Science, University College London.\n\n\n\n\n\n\nNote\n\n\n\nThis book is inspired by and adapted from Dr Andrew Maclahlan’s CASA0023 Book\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can click the “->” button below for the next chapter!"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Week 2 - Portfolio Tools",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Azarakhsh, Zeinab, Mohsen Azadbakht, and Aliakbar Matkan. 2022.\n“Estimation, Modeling, and Prediction of Land Subsidence Using\nSentinel-1 Time Series in Tehran-Shahriar\nPlain: A Machine Learning-Based Investigation.”\nRemote Sensing Applications: Society and Environment 25\n(January): 100691. https://doi.org/10.1016/j.rsase.2021.100691.\n\n\nBarsi, Á., Zs. Kugler, I. László, Gy. Szabó, and H. M. Abdulmutalib.\n2018. “ACCURACY DIMENSIONS IN REMOTE SENSING.”\nThe International Archives of the Photogrammetry, Remote Sensing and\nSpatial Information Sciences XLII-3 (April): 61–67. https://doi.org/10.5194/isprs-archives-XLII-3-61-2018.\n\n\nBlaschke, T. 2010. “Object Based Image Analysis for Remote\nSensing.” ISPRS Journal of Photogrammetry and Remote\nSensing 65 (1): 2–16. https://doi.org/10.1016/j.isprsjprs.2009.06.004.\n\n\n“Earth Engine Code Editor | Google Earth\nEngine.” n.d. Google Developers. Accessed\nMarch 24, 2023. https://developers.google.com/earth-engine/guides/playground.\n\n\n“Genangan Banjir Menggunakan\nNDWI.” n.d. Accessed March 24, 2023. https://alkindigifty.users.earthengine.app/view/genangan-banjir-menggunakan-ndwi.\n\n\nGISGeography. 2014. “Image Classification Techniques\nin Remote Sensing.” GIS Geography. May\n2, 2014. https://gisgeography.com/image-classification-techniques-remote-sensing/.\n\n\nHanami, Z. A., A. D. Damayanti, T. Takeda, and H. Alimuddin. 2022.\n“Estimation of Makassar’s Landfill Surface\nTemperature and Its Surroundings Using Remote\nSensing.” IOP Conference Series: Earth and\nEnvironmental Science 1117 (1): 012055. https://doi.org/10.1088/1755-1315/1117/1/012055.\n\n\n“JavaScript and Python Guides |\nGoogle Earth Engine | Google\nDevelopers.” n.d. Accessed March 24, 2023. https://developers.google.com/earth-engine/guides.\n\n\nLi, Aimin, and Guangping Xia. 2021. “The Influence of Geometric\nCorrection on the Accuracy of the Extraction of the Remote Sensing\nReflectance of Water.” International Journal of Remote\nSensing 42 (6): 2280–91. https://doi.org/10.1080/2150704X.2020.1847350.\n\n\nMaples, Stace. 2021. “Google Earth Engine\n101.” ArcGIS StoryMaps. May 13, 2021. https://storymaps.arcgis.com/stories/cdfc91d050634a5294ac897acc959d55.\n\n\nMehmood, Maryam, Ahsan Shahzad, Bushra Zafar, Amsa Shabbir, and Nouman\nAli. 2022. “Remote Sensing Image Classification:\nA Comprehensive Review and\nApplications.” Mathematical Problems in\nEngineering 2022 (August): e5880959. https://doi.org/10.1155/2022/5880959.\n\n\nNast, Condé. n.d. “The Impossible Fight to Save\nJakarta, the Sinking Megacity.” Wired UK.\nAccessed March 24, 2023. https://www.wired.co.uk/article/jakarta-sinking.\n\n\nUS EPA, OAR. 2014. “Learn About Heat Islands.”\nOverviews and Factsheets. June 17, 2014. https://www.epa.gov/heatislands/learn-about-heat-islands."
  },
  {
    "objectID": "intro.html#what-is-remote-sensing",
    "href": "intro.html#what-is-remote-sensing",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 What is Remote Sensing?",
    "text": "1.1 What is Remote Sensing?"
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 What is Remote Sensing?\nRemote sensing data is digital images and digital images are made of pixels, and pixels are representations of numeric values.\nThose values represent the amount of reflected electromagnetic energy (which is everywhere, and everything reflects or absorbs it differently, and measurably). The sunlight is hitting the earth and the earth reflects the light and satellites capture it.\nElectromagnetic energy has waves, have wavelength\n\n\n\nElectromagnetic Waves\n\n\nSource: Google Earth Engine 101 Stanford Geospatial Centre\nSpectral sampling\nRGB, Red Green Blue Spectrum\nreflect vs absorb\nin each spectral/spectrum, the image will be white if it reflects the spactrum\n255 spectrum of each RGB\nObserving the surface of the earth\nSatellites can see OUTSIDE RGB (more than 3 spectrum/band), such as infrared, panchromatic (black n white, take the intensity value), shortwave infrared, thermal infrared, etc.\nClassifying the pixels through the wavelength"
  },
  {
    "objectID": "intro.html#reflection",
    "href": "intro.html#reflection",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection"
  },
  {
    "objectID": "intro.html#conclusion",
    "href": "intro.html#conclusion",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.4 Conclusion",
    "text": "1.4 Conclusion"
  },
  {
    "objectID": "intro.html#applications",
    "href": "intro.html#applications",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\n1.2.1"
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.5 References",
    "text": "1.5 References"
  },
  {
    "objectID": "index.html#my-name",
    "href": "index.html#my-name",
    "title": "Remote Sensing Learning Diary",
    "section": "My name",
    "text": "My name\n\n\n\n\n\n\nNote\n\n\n\nYou can click the “->” button below for the next chapter!"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#FFC94A' , 'lineColor':'#327CA7'}}}%%\nflowchart LR;\n    id1[Remote Sensing]-->id11[Sensors];\n      id11-->id111[Types of Sensors];\n        id111-->id1111[UAVs and Drones];\n        id111-->id1112[Airplane and Helicopters];\n        id111-->id1113[Low Earth Orbit Satellites];\n      id11-->id112[Types of Resolutions];\n        id112-->id1121[Spatial Resolution];\n        id112-->id1122[Spectral Resolution];\n        id112-->id1123[Temporal Resolution];\n        id112-->id1124[Radiometric]\n      id11-->id115[Types of Orbits];\n        id115-->id1151[Geostationary];\n        id115-->id1152[Sun-synchronous];\n        id115-->id1153[Polar];\n    id1-->id12[Types of Remote Sensing];\n      id12-->id121[Active Sensor];\n      id12-->id122[Passive Sensor];\n    id1-->id13[Electromagnetic Spectrum];\n      id13-->id131[Spectral Bands];\n        id131-->id1311[Multispectral];\n        id131-->id1312[Hyperspectral];\n    id1-->id14[Image Classification];\n      id14-->id141[Supervised Classification];\n      id14-->id142[Unsupervised Classification];\n      id14-->id143[Object-based Image Analysis];\n    id1-->id15[Applications and Uses];\n      id15-->id151[Agriculture];\n      id15-->id152[Climate Change];\n      id15-->id153[Disasters];\n      id15-->id154[Environment];\n      id15-->id155[Urban];\n\n\n\n\n\n\n\n\n\n\nRemote sensing data is digital images and digital images are made of pixels, and pixels are representations of numeric values.\nThose values represent the amount of reflected electromagnetic energy (which is everywhere, and everything reflects or absorbs it differently, and measurably). The sunlight is hitting the earth and the earth reflects the light and satellites capture it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUAVs and Drones\nAirplanes and Helicopters\nLow Earth Orbit Satellites\n\n\n\n\nAdvantages\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n- High resolution imagery- Pilot-flown flight paths- LIDAR capabilities\n- High to coarse resolution imagery- Large coverage extent\n\n\nDisadvantages\n- Very small coverage extent- Visual line of sight\n- Small coverage extent- Flight operation\n- Coverage limited to orbital path- Cloud obstructions\n\n\n\n\n\n\n\n\n\nPassive and Active Sensors Source: NASA public domain in Balamis\n\n\n\n\n\nElectromagnetic energy has waves, have wavelength. According to Maples (2021), below is the illustration of electromagnetic waves.\n\n\n\nElectromagnetic Waves\n\n\nSource: Google Earth Engine 101 Stanford Geospatial Centre\nSpectral sampling\nRGB, Red Green Blue Spectrum\nreflect vs absorb\nin each spectral/spectrum, the image will be white if it reflects the spactrum\n255 spectrum of each RGB\nObserving the surface of the earth\nSatellites can see OUTSIDE RGB (more than 3 spectrum/band), such as infrared, panchromatic (black n white, take the intensity value), shortwave infrared, thermal infrared, etc (Maples 2021).\nClassifying the pixels through the wavelength"
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nExamples of applications of remote sensing in these sectors:\n\nAgriculture: Mapping crop types in the US Midwest Chapter A1.1\nClimate change: Air pollution and Population Exposure Chapter A1.4 using gridded air pollution data from Sentinel-5P.\nDisasters: Active fire monitoring Chapter A3.1\nEnvironment: Mangroves change mapping Chapter A3.3 using map-to-map change detection between 2000 and 2020.\nUrban: Impact of urban expansion (urban classification) Chapter A1.2"
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nI learned remote sensing in my undergraduate study almost 10 years ago and I realised there are so much changes happened. This module is like a refreshment and updated version of knowledge and technology that I learned in the past.\nThe advancement of the resolutions blows my mind. I used to work on Landsat 7’ image with 15-60m resolution, 7+1 bands, and 16 days temporal resolution. Mostly I used satellite images at my work in coal mining company for topographic map and DEM(Digital Elevation Model) to calculate the overburden removal and coal extraction volume.\nWith the newest technology, the improvement of remote sensing satellite such as Sentinel with better resolutions and Google Earth Engine makes different and broaden the type of analysis that we can do such as object-based image detection with better and faster result.\n\n\n\n\nKnuth, D. E. (1984) “Literate programming,” Comput. J., 27(2), pp. 97–111. doi: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week1.html#conclusion",
    "href": "week1.html#conclusion",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.4 Conclusion",
    "text": "1.4 Conclusion"
  },
  {
    "objectID": "week1.html#references",
    "href": "week1.html#references",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.5 References",
    "text": "1.5 References\n\n\nKnuth, D. E. (1984) ‘Literate programming’, Comput.\nJ., 27(2), pp. 97–111. doi: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Remote Sensing Learning Diary",
    "section": "Preface",
    "text": "Preface\nThis is a Learning Diary for MSc Urban Spatial Science CASA0023 Remotely Sensing Cities and Environment module.\nThis website contains weekly learning material with structures below:\n\nSummary\nApplications\nReflection"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Remote Sensing Learning Diary",
    "section": "About me",
    "text": "About me\nMy name is Rahmadita Listianingrum. I am an international student from Indonesia studying at MSc Urban Spatial Science, University College London. I am a former Mine Planning Engineer and graduated from Geodesy and Geomatics Engineering, Institut Teknologi Bandung. My current interest lies in data analysis and visualisation."
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Remote Sensing Learning Diary",
    "section": "How to use this book",
    "text": "How to use this book\nThis website is hosted on GitHub. Data used within this book is available online, however occasionally websites can undergo maintenance or be inaccessible due to political factors such as government shutdowns.\nTo get the most out of this book spend a few minutes learning how to control it, in the top right of this webpage you will see the following tools:\n\n\n search the entire book for a specific word\n control the side / menu bars\n share this book on linkedin, twitter, or facebook\n GitHub repository for this book\n my personal twitter account\n my linkedin profile\n\n\n\n\n\n\n\nAcknowledgement\n\n\n\nThis book is inspired by and adapted from Dr Andrew Maclahlan’s CASA0023 Book\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nYou can click the “->” button below to go to the next chapter\nYou can select the table of contents at the right hand side\n\n\n\n\n\n\n\n\n\nAttention\n\n\n\nThis book is in progress! I will update regularly"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Portfolio Tools",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "week2.html#summary",
    "href": "week2.html#summary",
    "title": "\n2  Portfolio Tools\n",
    "section": "\n2.1 Summary",
    "text": "2.1 Summary\n\nQuarto official siteBootswatch free bootsrap theme\n\nXaringan is a popular R packages for producing interactive and dynamic presentations. It is built on the renowned JavaScript library reveal.js and offers users a variety of customisation possibilities for creating entertaining and informative presentations.\nXaringan allows users to build presentations using the R Markdown syntax, which is a simple and straightforward markdown language that allows users to easily add code, photos, and other material in presentations.\nXaringan presentations can be rendered to HTML, making them easily shared and viewed on the internet. As a result, it is an excellent alternative for developing web presentations such as portfolios, project reports, and instructional resources."
  },
  {
    "objectID": "week2.html#applications",
    "href": "week2.html#applications",
    "title": "\n2  Portfolio Tools\n",
    "section": "\n2.2 Applications",
    "text": "2.2 Applications\nQuarto is used to create this remote sensing learning diary. I also have created the following slide presentation on one of the remote sensing sensors using the xaringan package:\nPortfolio tool application: xaringan\n\n\nRemote Sensing Sensor Slides\nLink\n\n\nSentinel - The Remote Sensing Sensor"
  },
  {
    "objectID": "week2.html#reflection",
    "href": "week2.html#reflection",
    "title": "\n2  Portfolio Tools\n",
    "section": "\n2.3 Reflection",
    "text": "2.3 Reflection\nI just knew the term “literate programming” after I entered CASA and I really enjoy it! In CASA0005 Geographic Information Systems and Science, I learned about Rmarkdown and I am getting used to take notes with markdown format with Obsidian. Quarto makes it even more convenient because it is integrated to several platforms such as RStudio and VSCode (I use both of these). Xaringan is on another level, it can generate Rmarkdown into slides! I just wanna say thank you to Andy, our lecturer, to encourage us to learn about this and makes us able to publish our work/portfolio. Such a useful lesson!"
  },
  {
    "objectID": "week2.html#conclusion",
    "href": "week2.html#conclusion",
    "title": "2  Portfolio Tools",
    "section": "2.4 Conclusion",
    "text": "2.4 Conclusion"
  },
  {
    "objectID": "week2.html#references",
    "href": "week2.html#references",
    "title": "2  Portfolio Tools",
    "section": "2.5 References",
    "text": "2.5 References"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Remote Sensing Data",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n\nThe woman behind multispectral band on satellite image/ERTS project (bringing visible RGB and invisible infrared, panc, etc) since 1972>> Landsat\n\n\n\nRBV: Return Beam Vidicon (TV-like cameras) >> system was analog MSS: Multi-Spectral Scanner \n\n\n\nPush broom: spotlight/across track scanners Whisk broom: along track scanners\n\n\n\nRemote sensing is like taking a picture of the Earth from a big camera in space. But sometimes, the picture doesn’t look right because of things like clouds, the way the camera is positioned, or other problems. So, we need to fix those problems so we can use the picture to learn more about the Earth. That’s what we call “remote sensing corrections”. It’s like making the picture better by changing things in it to make it look more like what the Earth really looks like.\n\nscan lines\n\nGeometric correction (parallax! remember photogrammetry in my undergrad)\nThis type of correction helps to fix any errors in the location or position of features in the image caused by the angle at which the satellite was positioned, the curvature of the Earth, and other factors.\n\nview angle\ntopography\nwind\nrotation of the earth\n\nGeometric correction helps to make sure that the features in the image are accurately located and aligned.\nGeometric correction on satellite vs airplane?\nHow to deal with? GCP: Ground Control Point linear regression: parallax - RMSE >> adding more GCPs can - Resample methods if the grid cells not align!: - Nearest neighbor - linear - cubic - cubic spline\nAtmospheric correction\nThe Earth’s atmosphere can absorb, reflect, and scatter radiation from the sun, which can affect the quality of the remote sensing image. Atmospheric correction removes these effects to reveal the true spectral values of the features in the image.\nIt is not necessary if you only have one single image or not a time-series tiles Necessary: have time elements (want to see the same spot in various times/time-series)\nAtmosphere will absorb/scatter the sunlight and create haze! Now there are: surface reflectance Analysis Ready Data (ARD)\n\n! bright object/really reflective object can cause to adjacency effect (one pixel influence others)\n\nHow to deal with? - Absolute (basically it is making a model of atmosphere) - FLAASH - Py6S (Python Second Simulation of the Satellite Signal in the Solar Spectrum) - need a lot of data/atmosphere element from station (e.g. ) - Relative: - DOS (Dark Object Subtraction) - PIFs (Psuedo-Invariant Features) determine feature that don’t change, make regression model per band:\nEmpirical line correction - using field spectometer\nIrradiance vs radiance\nOrthorectification correction / topographic correction\nWhen a satellite takes a picture of the Earth, it is viewing the planet from a very high altitude, and the images may appear distorted because of the angle at which they are taken. This can lead to inaccuracies in the image, such as features appearing stretched or compressed, or not being located in the correct place.\nCorrections: - cosine correction - mineart correction - statistical empirical correction - c correction (advancing the cosine)\ncosine correctional\n\nRadiometric calibration\nThis type of correction is used to adjust the brightness or contrast of the image so that it accurately represents the reflectance values of the features in the image. Radiometric correction helps to make sure that the features in the image are clearly visible and not too bright or too dark.\nSensors capture image brightness and are distributed as a Digital Number (or DN) - allows for efficient storage but has no units!\nDigitla Number (DN) to spectral radiance = radiometric calibration Radiometric calibration: Correct DN to spectral radiance\n\n\n\n\nimage stretch\nratio\nfusion: multi-temporal images or images from different satellites pan sharpening (use grey band/panchromatic to sharpen the rgb band)\nGEOBIA (Object-Based Image Analysis) \n\n\nThoughts: corrections\n\nImagery may contain error from a variety of sources\nWe must correct where appropriate \nWe must contextualise the use of the imagery \n\nThoughts: data joining and enhancement {.unnumbered}\n\nMosaicing in with a standard method isn’t appropriate for satellite imagery \nImagery can be “improved/enhanced” based on the energy reflected and the contrast between features \nBut…\n\nHow do these methods help in urban environments\nDoes adding complexity to imagery (or creating new datasets) assist us in our aim?"
  },
  {
    "objectID": "week3.html#applications",
    "href": "week3.html#applications",
    "title": "3  Remote Sensing Data",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nLi and Xia (2021) investigates the impact of geometric correction on the accuracy of extracting remote sensing reflectance (RSR) from water bodies using satellite imagery. The study found that geometric correction plays an essential role in the accurate retrieval of RSR. Using Landsat 8 Operational Land Imager (OLI) imagery acquired over the Taihu Lake region in China, which covered an area of approximately 180 km × 180 km with a spatial resolution of 30 m, the results show that the different geometric correction methods can cause significant variations in the retrieved RSR values, and the accuracy of the retrieved RSR can be significantly improved by applying a suitable geometric correction method.\nLimitation to this study is it did not consider other factors such as atmospheric correction or the impact of different algorithms on the accuracy of the extracted RSR values."
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Remote Sensing Data",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection"
  },
  {
    "objectID": "week3.html#conclusion",
    "href": "week3.html#conclusion",
    "title": "3  Remote Sensing Data",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion"
  },
  {
    "objectID": "week3.html#references",
    "href": "week3.html#references",
    "title": "3  Remote Sensing Data",
    "section": "3.5 References",
    "text": "3.5 References"
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  Policy Application",
    "section": "4.1 Summary",
    "text": "4.1 Summary\n Source: Wired UK Nast (n.d.)\n\n\nAs the sea level rise and groundwater extraction increasing, the future of coastal cities are in danger.\nLet’s take a look in Jakarta Metropolitan City, the capital city of Indonesia. Quick facts about Jakarta:\n\nPopulation (estimate): 10,562,088\nArea: 664.01 km2\nGeographical situation: Coastal city in the south of Java Sea\nRisk: Flood, earthquake, land subsidence\n\n\n\n\nJakarta is sinking in 2030! due to the global sea level rise and groundwater extraction.\n\n\n\nCurrent policy\nMari Simak Peraturan Larangan Penggunaan Air Tanah di DKI Jakarta\nGroundwater, key to the Sustainable Development Goals (SDGs)\nIndonesian government set the restriction of groundwater extraction by different fees , will it work?\nThe problem is, the Jakarta’s government only restricted CBD area around Sudirman and the policy about extraction restriction is like this:\n\nExtraction restriction only applied in the designated area (around Sudirman CBD), Central Jakarta.\nThe restriction only applied for the above 5 stories building.\nThe restriction is constrain only by the price of groundwater extraction.\nBuilding owner have to put groundwater tools monitoring in their building.\n\nI am focusing on groundwater extraction and the number 1 issue which is the planned area of restriction only applied in the Central Jakarta without consideration of data.\nSo, I set the research question: How is data-driven approach (using remote sensing) can help optimise the solution for Jakarta’s land subsidence issue? The solution is in the application below"
  },
  {
    "objectID": "week4.html#methods",
    "href": "week4.html#methods",
    "title": "4  Policy Application",
    "section": "4.2 ### Methods",
    "text": "4.2 ### Methods"
  },
  {
    "objectID": "week4.html#solutions",
    "href": "week4.html#solutions",
    "title": "4  Policy Application",
    "section": "4.3 ### Solutions",
    "text": "4.3 ### Solutions\n\n4.3.1 Limitations and Future\n\nAccuracy\nFuture: NISAR (NASA-ISRO SAR EO satellite launch in 2024) with 2 bands (L-band and S-band)"
  },
  {
    "objectID": "week4.html#applications",
    "href": "week4.html#applications",
    "title": "4  Policy Application",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nUsing remote sensing to model and predict the land subsidence is useful to decide which area should be restricted to groundwater extraction in Jakarta.\nAccording to Azarakhsh, Azadbakht, and Matkan (2022) reserach paper, they use below data to model and predict land subsidence: - InSAR Sentinel-1 - In this study, a total number of 193 Sentinel-1A images acquired between October 8, 2014 and March 28, 2019 were used. - Additionally, they used the Shuttle Radar Topography Mission (SRTM) Digital Elevation Model (DEM) for preliminary geocoding and removing the topographic phase component.\nIn the former category, water flow and soil mechanics have been modeled through simulating the process of LS in aquifer systems according to groundwater abstraction and geological, geotechnical and hydrogeological conditions (Fernandez et al., 2018; Bajni et al., 2019; Zhu et al., 2020). In such methods, it is necessary to access accurate hydrogeological data and adopt various assumptions (Ilia et al., 2018).\nSeveral studies have also applied Persistent Scatterer Interferometry (PSI) techniques and statistical-empirical methods such as back propagation neural networks (BPNs) (Dehghani et al., 2013), long short-term memory (LSTM) (Li et al., 2020), geographically weighted LSTM (GW-LSTM) (Li et al., 2021), Random forest (RF) (Ilia et al., 2018), analytical hierarchy process coupled with sensitivity analysis (AHP-SA) (Zhu et al., 2013), and GIS based spatial analysis (Zhu et al., 2015) to model and predict LS.\nThe geological and hydrological parameters considered for LS modeling using the ML methods include groundwater level change, aquifer media, precipitation rate, slope, land use, depth to water table, distance from exploiting wells, distance from rivers, and distance from faults.\nAll these layers were converted to raster, and their values were extracted to PS points detected by PS-InSAR analysis as sample points. The effects of these parameters on LS were investigated and the most important parameters were determined.\nIn this paper, Azarakhsh, Azadbakht, and Matkan (2022) applied the PS-InSAR technique proposed by Ferretti et al. (2001) to monitor LS at millimeter precision (Perissin, 2016) over the study period. This technique is based on detection of points (or targets) called the permanent scatterers (PS) that remain unchanged over the entire period. For PS-InSAR analysis, single look complex (SLC) images of the same area were co-registered to a single master configuration preferably selected from the middle of the spatial and temporal baseline spaces (Hanssen, 2001). Interferograms were then generated from multi-temporal SAR images. Finally, the orbital data and the external SRTM DEM were used to remove the Earth curvature influence and the topographic component of the interferometric phase, respectively.\nIn conclusion, this study combine: PS-InSAR (mm precision), ANN, and RF to model the landsubsidence.\nLimitations and works\n\nTo validate the estimated LS measurements, it is necessary that the vertical displacement is compared with the GPS measurements.\nAccuracy\nFuture: using NISAR data (NASA-ISRO SAR EO satellite launch in 2024) with 2 bands (L-band and S-band)"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  Policy Application",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection"
  },
  {
    "objectID": "week4.html#conclusion",
    "href": "week4.html#conclusion",
    "title": "4  Policy Application",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion"
  },
  {
    "objectID": "week4.html#references",
    "href": "week4.html#references",
    "title": "4  Policy Application",
    "section": "4.4 References",
    "text": "4.4 References"
  },
  {
    "objectID": "week4.html#conclusion-and-reflection",
    "href": "week4.html#conclusion-and-reflection",
    "title": "4  Policy Application",
    "section": "4.3 Conclusion and Reflection",
    "text": "4.3 Conclusion and Reflection"
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nGoogle Earth Engine (GEE) is a cloud-based platform for analyzing and processing geospatial data. It allows users to access and process massive amounts of remote sensing data for various purposes, including land use and land cover analysis, natural resource management, and disaster response. GEE has a powerful processing infrastructure that allows users to apply various geospatial analysis techniques, such as image classification, object detection, and time-series analysis.\nUser Interface (UI) of GEE “Earth Engine Code Editor | Google Earth Engine” (n.d.):\n\n\n\n\nTerms/Jargon Specific to GEE GEE has several terms and jargon that are specific to the platform. For example, ee.Image is a class in the GEE API used to represent an image, ee.ImageCollection is used to represent a collection of images, and ee.Geometry is used to represent geometries like points, lines, and polygons. Other terms include ee.Reducer, ee.Filter, and ee.FeatureCollection. A complete list of GEE-specific terms and jargon can be found in the GEE documentation.\nRelating Spatial Data Formats We Have Seen to GEE GEE can work with various spatial data formats, including GeoTIFF, NetCDF, and SHP. These formats can be uploaded to the GEE platform and processed using GEE’s tools and algorithms. GEE also supports several remote sensing data formats, including Landsat, Sentinel-2, MODIS, and more.\nClient vs. Server-Side GEE processing can occur on both the client-side and server-side. The client-side involves using GEE’s JavaScript API to send requests to the server, visualize data, and process small-scale data. On the other hand, the server-side involves using GEE’s computing infrastructure to process large-scale geospatial data. The server-side can process data more efficiently and effectively than the client-side, making it ideal for large-scale analyses.\nScale (Resolution) GEE works with geospatial data at various scales or resolutions. The platform supports data with a spatial resolution ranging from a few meters to several kilometers. The choice of scale depends on the user’s analysis goals and the spatial and temporal extent of the data.\nProjections GEE supports various map projections, including Geographic, UTM, and Web Mercator. The platform automatically applies the correct projection to the data, ensuring that the data is displayed correctly on the map.\n\n\n\n\nGEE can be accessed through the Google Earth Engine Code Editor, where users can write and run JavaScript code to access and analyze geospatial data. Users can also use GEE’s pre-built tools, such as the Image and Feature Editors, to view and edit geospatial data directly!!\n\nBuilding blocks of data (the data in GEE) The building blocks of data in GEE are images and image collections. Images are raster data with multiple bands, such as satellite imagery, and can be processed using GEE’s built-in functions. Image collections are collections of images, and they can be filtered, sorted, and processed as a group.\nCollections, geometries, and features In GEE, collections are groups of objects, such as a collection of satellite images or a collection of point locations. Geometries are shapes used to define regions of interest, such as a polygon for a city boundary. Features are objects that contain geometries and properties, such as a point location with attributes like temperature and humidity.\nReducing images (e.g., zonal statistics) Reducing images involves computing a statistic or value for each pixel or region in an image. For example, zonal statistics can be used to calculate the mean temperature for each polygon in a feature collection. GEE has built-in functions for reducing images, such as ee.Reducer.mean().\nRegression (over time) Regression can be used to analyze changes in geospatial data over time. For example, linear regression can be used to analyze changes in vegetation over time using satellite imagery. GEE has built-in functions for regression, such as ee.Reducer.linearRegression().\nJoins Joins involve combining data from two or more collections based on a common attribute. For example, a feature collection of cities can be joined with a table of population data based on a common attribute such as city name. GEE has built-in functions for joining data, such as ee.Join.inner().\nMachine learning (intro) GEE also supports machine learning algorithms, such as classification and clustering. Machine learning can be used to classify land cover types using satellite imagery or to identify patterns in geospatial data. GEE has built-in functions for machine learning, such as ee.Classifier.svm().\n\nIn summary, GEE can be used through the Code Editor or pre-built tools to access and analyze geospatial data. The building blocks of data in GEE are images and image collections, and GEE supports collections, geometries, and features. GEE also supports reducing images, regression over time, joins, and machine learning algorithms (“JavaScript and Python Guides | Google Earth Engine | Google Developers” n.d.)."
  },
  {
    "objectID": "week5.html#applications",
    "href": "week5.html#applications",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nI took a look into one of the GEE applications from this Awesome Earth Engine Apps github repository, and found the study case in Indonesia!\nGenangan banjir menggunakan NDWI Which means: Flood inundation using NDWI\nAccording to “Genangan Banjir Menggunakan NDWI” (n.d.), this GEE apps view and analyze flood inundation using the Normalized Difference Water Index (NDWI) for a specific area (East Java, Indonesia).\n\n\n\nThe application uses NDWI, a remote sensing index that can detect water bodies’ presence and extent.\nNDWI is calculated using near-infrared and shortwave-infrared bands of satellite imagery.\nThe user selects an area of interest on the map, and the application loads satellite imagery for that area.\nThen calculates the NDWI for the selected area and displays it as a map layer.\nThe user can adjust the threshold value for the NDWI layer to highlight different water levels, allowing them to see areas that are potentially flooded. 6. The application also provides a time series of NDWI values, allowing the user to observe changes in water levels over time.\n\nClearly, this application demonstrate the potential of remote sensing for flood mapping and monitoring, which can be useful for disaster response and management (e.g. flood monitoring and early warning, water management, and environmental monitoring).\nOther applications on GEE?\nMany!\nIncluding land cover and land use change detection, climate change analysis, deforestation monitoring, and crop yield estimation."
  },
  {
    "objectID": "week5.html#conclusion-and-reflection",
    "href": "week5.html#conclusion-and-reflection",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "5.3 Conclusion and Reflection",
    "text": "5.3 Conclusion and Reflection\n\n\n\n\nKnuth, D. E. (1984) “Literate programming,” Comput. J., 27(2), pp. 97–111. doi: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week5.html#references",
    "href": "week5.html#references",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "References",
    "text": "References\n\n\nKnuth, D. E. (1984) “Literate programming,” Comput. J., 27(2), pp. 97–111. doi: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Remote Sensing Data",
    "section": "",
    "text": "4 Corrections\nMAKE AN RS DICTIONARY (zenith, azimuth)\nLandsat Data Access\nLandsat USGS > comes with collections: Landsat Collections ensure that all Landsat Level-1 products contain known data quality. Source  Level means specific product (temp, humidity, etc)\nExample: Landsat 8-9 Operational Land Imager (OLI) and Thermal Infrared (TIRS) Collection 2 Level-2 Science Products 30-meter multispectral data. Source"
  },
  {
    "objectID": "week3.html#geometric-correction-parallax-remember-photogrammetry",
    "href": "week3.html#geometric-correction-parallax-remember-photogrammetry",
    "title": "3  Remote Sensing Data",
    "section": "4.1 Geometric correction (parallax! remember photogrammetry)",
    "text": "4.1 Geometric correction (parallax! remember photogrammetry)\n\nview angle\ntopography\nwind\nrotation of the earth\n\nGeometric correction on satellite vs airplane?\nHow to deal with? GCP: Ground Control Point linear regression: parallax - RMSE >> adding more GCPs can - Resample methods if the grid cells not align!: - Nearest neighbor - linear - cubic - cubic spline"
  },
  {
    "objectID": "week3.html#atmospheric-correction",
    "href": "week3.html#atmospheric-correction",
    "title": "3  Remote Sensing Data",
    "section": "4.2 Atmospheric correction",
    "text": "4.2 Atmospheric correction\nNot necessary if you only have one single image or not a time-series tiles Necessary: have time elements (want to see the same spot in various times/time-series)\nAtmosphere will absorb/scatter the sunlight and create haze! Now there are: surface reflectance Analysis Ready Data (ARD)\n\n! bright object/really reflective object can cause to adjacency effect (one pixel influence others)\n\nHow to deal with? - Absolute (basically it is making a model of atmosphere) - FLAASH - Py6S (Python Second Simulation of the Satellite Signal in the Solar Spectrum) - need a lot of data/atmosphere element from station (e.g. ) - Relative: - DOS (Dark Object Subtraction) - PIFs (Psuedo-Invariant Features) determine feature that don’t change, make regression model per band:\nEmpirical line correction - using field spectometer\nIrradiance vs radiance"
  },
  {
    "objectID": "week3.html#orthorectification-correction-topographic-correction",
    "href": "week3.html#orthorectification-correction-topographic-correction",
    "title": "3  Remote Sensing Data",
    "section": "4.3 Orthorectification correction / topographic correction",
    "text": "4.3 Orthorectification correction / topographic correction\n\ncosine correction\nmineart correction\nstatistical empirical correction\nc correction (advancing the cosine)\n\ncosine correctional"
  },
  {
    "objectID": "week3.html#radiometric-calibration",
    "href": "week3.html#radiometric-calibration",
    "title": "3  Remote Sensing Data",
    "section": "4.4 Radiometric calibration",
    "text": "4.4 Radiometric calibration\nSensors capture image brightness and are distributed as a Digital Number (or DN) - allows for efficient storage but has no units!\nDN to spectral radiance = radiometric calibration Radiometric calibration: Correct DN to spectral radiance"
  },
  {
    "objectID": "week3.html#mosaicking",
    "href": "week3.html#mosaicking",
    "title": "3  Remote Sensing Data",
    "section": "5.1 Mosaicking",
    "text": "5.1 Mosaicking"
  },
  {
    "objectID": "week3.html#image-enhancement",
    "href": "week3.html#image-enhancement",
    "title": "3  Remote Sensing Data",
    "section": "5.2 Image enhancement",
    "text": "5.2 Image enhancement\n\nimage stretch\nratio\nfusion: multi-temporal images or images from different satellites pan sharpening (use grey band/panchromatic to sharpen the rgb band)\nGEOBIA (Object-Based Image Analysis)"
  },
  {
    "objectID": "week3.html#corrections",
    "href": "week3.html#corrections",
    "title": "3  Remote Sensing Data",
    "section": "Corrections",
    "text": "Corrections\nRemote sensing is like taking a picture of the Earth from a big camera in space. But sometimes, the picture doesn’t look right because of things like clouds, the way the camera is positioned, or other problems. So, we need to fix those problems so we can use the picture to learn more about the Earth. That’s what we call “remote sensing corrections”. It’s like making the picture better by changing things in it to make it look more like what the Earth really looks like.\n\nscan lines\n\n\nGeometric correction (parallax! remember photogrammetry in my undergrad)\nThis type of correction helps to fix any errors in the location or position of features in the image caused by the angle at which the satellite was positioned, the curvature of the Earth, and other factors.\n\nview angle\ntopography\nwind\nrotation of the earth\n\nGeometric correction helps to make sure that the features in the image are accurately located and aligned.\nGeometric correction on satellite vs airplane?\nHow to deal with? GCP: Ground Control Point linear regression: parallax - RMSE >> adding more GCPs can - Resample methods if the grid cells not align!: - Nearest neighbor - linear - cubic - cubic spline\n\n\nAtmospheric correction\nThe Earth’s atmosphere can absorb, reflect, and scatter radiation from the sun, which can affect the quality of the remote sensing image. Atmospheric correction removes these effects to reveal the true spectral values of the features in the image.\nIt is not necessary if you only have one single image or not a time-series tiles Necessary: have time elements (want to see the same spot in various times/time-series)\nAtmosphere will absorb/scatter the sunlight and create haze! Now there are: surface reflectance Analysis Ready Data (ARD)\n\n! bright object/really reflective object can cause to adjacency effect (one pixel influence others)\n\nHow to deal with? - Absolute (basically it is making a model of atmosphere) - FLAASH - Py6S (Python Second Simulation of the Satellite Signal in the Solar Spectrum) - need a lot of data/atmosphere element from station (e.g. ) - Relative: - DOS (Dark Object Subtraction) - PIFs (Psuedo-Invariant Features) determine feature that don’t change, make regression model per band:\nEmpirical line correction - using field spectometer\nIrradiance vs radiance\n\n\nOrthorectification correction / topographic correction\nWhen a satellite takes a picture of the Earth, it is viewing the planet from a very high altitude, and the images may appear distorted because of the angle at which they are taken. This can lead to inaccuracies in the image, such as features appearing stretched or compressed, or not being located in the correct place.\nCorrections: - cosine correction - mineart correction - statistical empirical correction - c correction (advancing the cosine)\ncosine correctional \n\n\n\nRadiometric calibration\nThis type of correction is used to adjust the brightness or contrast of the image so that it accurately represents the reflectance values of the features in the image. Radiometric correction helps to make sure that the features in the image are clearly visible and not too bright or too dark.\nSensors capture image brightness and are distributed as a Digital Number (or DN) - allows for efficient storage but has no units!\nDigitla Number (DN) to spectral radiance = radiometric calibration Radiometric calibration: Correct DN to spectral radiance"
  },
  {
    "objectID": "week3.html#data-joining-and-enhancement",
    "href": "week3.html#data-joining-and-enhancement",
    "title": "3  Remote Sensing Data",
    "section": "Data joining and enhancement",
    "text": "Data joining and enhancement\n\nMosaicking\n\n\nImage enhancement\n\nimage stretch\nratio\nfusion: multi-temporal images or images from different satellites pan sharpening (use grey band/panchromatic to sharpen the rgb band)\nGEOBIA (Object-Based Image Analysis) \n\n\nThoughts: corrections\n\nImagery may contain error from a variety of sources\nWe must correct where appropriate \nWe must contextualise the use of the imagery \n\nThoughts: data joining and enhancement {.unnumbered}\n\nMosaicing in with a standard method isn’t appropriate for satellite imagery \nImagery can be “improved/enhanced” based on the energy reflected and the contrast between features \nBut…\n\nHow do these methods help in urban environments\nDoes adding complexity to imagery (or creating new datasets) assist us in our aim?"
  },
  {
    "objectID": "week3.html#summary-1",
    "href": "week3.html#summary-1",
    "title": "3  Remote Sensing Data",
    "section": "Summary",
    "text": "Summary\n\nPart 1: corrections\n\nImagery may contain error from a variety of sources\nWe must correct where appropriate \nWe must contextualise the use of the imagery \n\n\n\nPart 2: data joining and enhancement\n\nMosaicing in with a standard method isn’t appropriate for satellite imagery \nImagery can be “improved/enhanced” based on the energy reflected and the contrast between features \nBut…\n\nHow do these methods help in urban environments\nDoes adding complexity to imagery (or creating new datasets) assist us in our aim?\n\n\n\n\nTask\nMAKE AN RS DICTIONARY (zenith, azimuth)"
  },
  {
    "objectID": "week3.html#practical3",
    "href": "week3.html#practical3",
    "title": "3  Remote Sensing Data",
    "section": "Practical3",
    "text": "Practical3\nLandsat Data Access\nLandsat USGS > comes with collections: Landsat Collections ensure that all Landsat Level-1 products contain known data quality. Source  Level means specific product (temp, humidity, etc)\nExample: Landsat 8-9 Operational Land Imager (OLI) and Thermal Infrared (TIRS) Collection 2 Level-2 Science Products 30-meter multispectral data. Source"
  },
  {
    "objectID": "week4.html#personal-reflection",
    "href": "week4.html#personal-reflection",
    "title": "4  Policy Application",
    "section": "4.3 Personal Reflection",
    "text": "4.3 Personal Reflection\nAs serious as it is, the government works slowly and tend to abandon the fact that Jakarta is sinking. Instead, they move the nation’s capital into Nusantara, Kalimantan island. This issue made me realise the importance of data and study before deciding a policy. It looks makes sense at first when they assigned the area of restriction in the Central Jakarta, because it is the most densed area in Jakarta. However, looking at North Jakarta that are already sinking (some areas) and given Jakarta’s geography, the decision of this policy must take land subsidence data and modeling into account. Furthermore, if such study has conducted, we can predict the future of Jakarta city. Hopefully the government can work faster and make data-driven policy.\n\n\n\n\nAzarakhsh, Zeinab, Mohsen Azadbakht, and Aliakbar Matkan. 2022. “Estimation, Modeling, and Prediction of Land Subsidence Using Sentinel-1 Time Series in Tehran-Shahriar Plain: A Machine Learning-Based Investigation.” Remote Sensing Applications: Society and Environment 25 (January): 100691. https://doi.org/10.1016/j.rsase.2021.100691.\n\n\nNast, Condé. n.d. “The Impossible Fight to Save Jakarta, the Sinking Megacity.” Wired UK. Accessed March 24, 2023. https://www.wired.co.uk/article/jakarta-sinking."
  },
  {
    "objectID": "week3.html#personal-reflection",
    "href": "week3.html#personal-reflection",
    "title": "3  Remote Sensing Data",
    "section": "3.3 Personal Reflection",
    "text": "3.3 Personal Reflection\nMy thoughts on this week’s lecture is although there is already corrected data, we still need to know about the fundamental and principle study behind the image correction in remote sensing. in my undergrad study, the image correction is a crucial step in obtaining accurate and reliable data from satellite imagery.\n\n\n\n\nLi, Aimin, and Guangping Xia. 2021. “The Influence of Geometric Correction on the Accuracy of the Extraction of the Remote Sensing Reflectance of Water.” International Journal of Remote Sensing 42 (6): 2280–91. https://doi.org/10.1080/2150704X.2020.1847350."
  },
  {
    "objectID": "week2.html#personal-reflection",
    "href": "week2.html#personal-reflection",
    "title": "\n2  Portfolio Tools\n",
    "section": "\n2.3 Personal Reflection",
    "text": "2.3 Personal Reflection\nI just learned the term “literate programming” after I started studying at CASA, and I love it! I learned about Rmarkdown in CASA0005 Geographic Information Systems and Science, and I’m becoming used to taking notes in markdown style with Obsidian. Quarto makes it even easier because it integrates with multiple platforms, including RStudio and VSCode (I use both of these). Xaringan takes things to the next level by converting Rmarkdown into presentations! I’d want to thank Andy, our lecturer, for encouraging us to learn about this and post our work/portfolio. What a valuable lesson!"
  },
  {
    "objectID": "week1.html#personal-reflection",
    "href": "week1.html#personal-reflection",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Personal Reflection",
    "text": "1.3 Personal Reflection\nI first learned about remote sensing during my undergraduate studies about ten years ago, and I’m amazed at how much it has changed. This module is a refresher and updated version of knowledge and technologies that I previously learned.\nMy mind is blown by how far the resolutions have advanced. I used to work on a 16-day temporal resolution, 7+1 bands, and 15–60 m spatial resolution on Landsat 7’ image. At my job in the mining industry, I mostly used satellite images to create topographic maps and calculate the volume of coal extraction and overburden removal using a DEM (Digital Elevation Model).\nWith the most recent technology, the improvement of remote sensing satellites such as Sentinel with higher resolutions and Google Earth Engine allows us to do different and more extensive analyses such as object-based image detection with better and faster results.\n\n\n\n\nMaples, Stace. 2021. “Google Earth Engine 101.” ArcGIS StoryMaps. May 13, 2021. https://storymaps.arcgis.com/stories/cdfc91d050634a5294ac897acc959d55."
  },
  {
    "objectID": "week5.html#personal-reflection",
    "href": "week5.html#personal-reflection",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "5.3 Personal Reflection",
    "text": "5.3 Personal Reflection\nI guess GEE is a game-changer in remote sensing industry. My thought about Why it’s different?:\n\nThe GEE platform is unique because it provides access to a massive collection of geospatial datasets, tools, and algorithms in a single platform, eliminating the need for users to search and download data from multiple sources! (I remember how hard this was 10 years ago lol)\nThe GEE platform provides a scalable and flexible infrastructure for processing and analyzing large-scale geospatial data, enabling users to conduct complex analyses quickly and efficiently.\nThe platform is also accessible to anyone (with an internet connection, of course!), making it an excellent tool for democratizing geospatial analysis.\n\n\n\n\n\n“Earth Engine Code Editor | Google Earth Engine.” n.d. Google Developers. Accessed March 24, 2023. https://developers.google.com/earth-engine/guides/playground.\n\n\n“Genangan Banjir Menggunakan NDWI.” n.d. Accessed March 24, 2023. https://alkindigifty.users.earthengine.app/view/genangan-banjir-menggunakan-ndwi.\n\n\n“JavaScript and Python Guides | Google Earth Engine | Google Developers.” n.d. Accessed March 24, 2023. https://developers.google.com/earth-engine/guides."
  },
  {
    "objectID": "ms.html",
    "href": "ms.html",
    "title": "9  Assessment",
    "section": "",
    "text": "Assessment1\nGroup code\nYou should not just restate what has been presented in the lectures\nMermaid mindmap (error live demo version)"
  },
  {
    "objectID": "ms.html#marking-scheme",
    "href": "ms.html#marking-scheme",
    "title": "9  Assessment",
    "section": "Marking scheme",
    "text": "Marking scheme\n\n1. Problem definition\nThe research problem is globally or highly pertinent or topical to a social, environmental, political or other context and has been framed at the optimal spatial scale. There is extensive background research evaluating multiple forms of credible sources to produce a compelling and coherent framing of the research question, presented in a concise and informative manner.\n\n\n2. Approach\nSelected data and presented analytical methodology are able to perfectly solve the presented research question or the limitations are evidently understood and expertly expressed. The analysis is entirely appropriate to the problem. It demonstrates creativity, a comprehensive understanding of appropriate sophisticated techniques, exemplary technical proficiency and skills which are indicative of significant additional independent learning.\n\n\n3. Project plan, risks and value for money\nThe project presents an exemplary plan. It is robust, logical, appropriate, and well informed, containing no ambiguity. It is evident how the deliverables will be useful to the city and could be incorporated into business as usual operations, this may include future areas of work or city challenges. All likely risks to the project have been identified and appropriate mitigative actions devised. Value for money in all aspects of the project is achieved, whilst producing tangible and actionable outputs."
  },
  {
    "objectID": "ms.html#marking-scheme-1",
    "href": "ms.html#marking-scheme-1",
    "title": "9  Assessment",
    "section": "Marking scheme",
    "text": "Marking scheme\n\n1. Content summary\nA well-defined summary / introduction that expertly demonstrates clear, unambiguous and extensive comprehension of the data, methods and tools/policies*, their benefits, limitations and possible future developments.\n*policies where appropriate\n\n\n2. Applications of the content\nDiscussed applications of the taught content are systematic, extensive and evaluate several credible sources using an in- depth and thorough narrative. It seamlessly guides the reader through both how and why the methods / data/ tools have been applied whilst providing exemplary critical reflection on the approaches with reference to future literature advancements.\n\n\n3. Personal reflection\nAn in depth and extensive personal reflection that demonstrates why the content may (or may not) be interesting, and presents an inquisitive dialogue to a wider application of the content. The skills, content, data and tools and placed within the broader discipline and it is evident how they could be used in future or may (or may not) lead to other avenues of analysis / work that could involve different datasets."
  },
  {
    "objectID": "week1.html#overview",
    "href": "week1.html#overview",
    "title": "1  Introduction to Remote Sensing",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#FFC94A' , 'lineColor':'#327CA7'}}}%%\nflowchart TD;\n    id1[Remote Sensing]-->id11[Sensors];\n      id11-->id111[Types of Sensors];\n        id111-->id1111[UAVs and Drones];\n        id111-->id1112[Airplane and Helicopters];\n        id111-->id1113[Low Earth Orbit Satellites];\n      id11-->id112[Types of Resolutions];\n        id112-->id1121[Spatial Resolution];\n        id112-->id1122[Spectral Resolution];\n        id112-->id1123[Temporal Resolution];\n        id112-->id1124[Radiometric]\n      id11-->id115[Types of Orbits];\n        id115-->id1151[Geostationary];\n        id115-->id1152[Sun-synchronous];\n        id115-->id1153[Polar];\n    id1-->id12[Types of Remote Sensing];\n      id12-->id121[Active Sensor];\n      id12-->id122[Passive Sensor];\n    id1-->id13[Electromagnetic Spectrum];\n      id13-->id131[Spectral Bands];\n        id131-->id1311[Multispectral];\n        id131-->id1312[Hyperspectral];\n    id1-->id14[Image Classification];\n      id14-->id141[Supervised Classification];\n      id14-->id142[Unsupervised Classification];\n      id14-->id143[Object-based Image Analysis];\n    id1-->id15[Applications and Uses];\n      id15-->id151[Agriculture];\n      id15-->id152[Climate Change];\n      id15-->id153[Disasters];\n      id15-->id154[Environment];\n      id15-->id155[Urban];\n\n\n\n\n\n\n\n\n\n\nRemote sensing data is digital images and digital images are made of pixels, and pixels are representations of numeric values.\nThose values represent the amount of reflected electromagnetic energy (which is everywhere, and everything reflects or absorbs it differently, and measurably). The sunlight is hitting the earth and the earth reflects the light and satellites capture it.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUAVs and Drones\nAirplanes and Helicopters\nLow Earth Orbit Satellites\n\n\n\n\nAdvantages\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n\n\nDisadvantages\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n- Very high resolution imagery- Programmable flight paths- LIDAR capabilities\n\n\n\n\n\n\n\n\n\nPassive and Active Sensors Source: NASA public domain in Balamis\n\n\n\n\n\nElectromagnetic energy has waves, have wavelength. According to Maples (2021), below is the illustration of electromagnetic waves.\n\n\n\nElectromagnetic Waves\n\n\nSource: Google Earth Engine 101 Stanford Geospatial Centre\nSpectral sampling\nRGB, Red Green Blue Spectrum\nreflect vs absorb\nin each spectral/spectrum, the image will be white if it reflects the spactrum\n255 spectrum of each RGB\nObserving the surface of the earth\nSatellites can see OUTSIDE RGB (more than 3 spectrum/band), such as infrared, panchromatic (black n white, take the intensity value), shortwave infrared, thermal infrared, etc (Maples 2021).\nClassifying the pixels through the wavelength"
  },
  {
    "objectID": "week3.html#overview",
    "href": "week3.html#overview",
    "title": "3  Remote Sensing Data",
    "section": "Overview",
    "text": "Overview\n\nReferences\nPennsylvania State University - GIS\n\n\nVirginia Norwood\nThe woman behind multispectral band on satellite image/ERTS project (bringing visible RGB and invisible infrared, panc, etc) since 1972>> Landsat\n\n\nRBV vs MSS\nRBV: Return Beam Vidicon (TV-like cameras) >> system was analog MSS: Multi-Spectral Scanner \n\n\nPush broom vs Whisk broom\nPush broom: spotlight/across track scanners Whisk broom: along track scanners"
  },
  {
    "objectID": "week4.html#overview",
    "href": "week4.html#overview",
    "title": "4  Policy Application",
    "section": "Overview",
    "text": "Overview\n\nStudy Case\nAs the sea level rise and groundwater extraction increasing, the future of coastal cities are in danger.\nLet’s take a look in Jakarta Metropolitan City, the capital city of Indonesia. Quick facts about Jakarta:\n\nPopulation:\nArea:\nWater catchment area:\nGeographical situation:\nRisk:\n\nReference: Estimation, modeling, and prediction of land subsidence using Sentinel-1 time series in Tehran-Shahriar plain: A machine learning-based investigation\nRelated issues\n\nClean water\n\nRelated global initiative\nSDGs\n\n\nData\n\nInSAR Sentinel-1\nIn this study, a total number of 193 Sentinel-1A images acquired between October 8, 2014 and March 28, 2019 were used.\nAdditionally, we used the Shuttle Radar Topography Mission (SRTM) Digital Elevation Model (DEM) for preliminary geocoding and removing the topographic phase component.\n\n\n\nMethods\n\nIn the former category, water flow and soil mechanics have been modeled through simulating the process of LS in aquifer systems according to groundwater abstraction and geological, geotechnical and hydrogeological conditions (Fernandez et al., 2018; Bajni et al., 2019; Zhu et al., 2020). In such methods, it is necessary to access accurate hydrogeological data and adopt various assumptions (Ilia et al., 2018).\nSeveral studies have applied Persistent Scatterer Interferometry (PSI) techniques and statistical-empirical methods such as back propagation neural networks (BPNs) (Dehghani et al., 2013), long short-term memory (LSTM) (Li et al., 2020), geographically weighted LSTM (GW-LSTM) (Li et al., 2021), Random forest (RF) (Ilia et al., 2018), analytical hierarchy process coupled with sensitivity analysis (AHP-SA) (Zhu et al., 2013), and GIS based spatial analysis (Zhu et al., 2015) to model and predict LS.\nThe geological and hydrological parameters considered for LS modeling using the ML methods include groundwater level change, aquifer media, precipitation rate, slope, land use, depth to water table, distance from exploiting wells, distance from rivers, and distance from faults.\nAll these layers were converted to raster, and their values were extracted to PS points detected by PS-InSAR analysis as sample points. The effects of these parameters on LS were investigated and the most important parameters were determined.\nIn this paper, we applied the PS-InSAR technique proposed by Ferretti et al. (2001) to monitor LS at millimeter precision (Perissin, 2016) over the study period. This technique is based on detection of points (or targets) called the permanent scatterers (PS) that remain unchanged over the entire period. For PS-InSAR analysis, single look complex (SLC) images of the same area were co-registered to a single master configuration preferably selected from the middle of the spatial and temporal baseline spaces (Hanssen, 2001). Interferograms were then generated from multi-temporal SAR images. Finally, the orbital data and the external SRTM DEM were used to remove the Earth curvature influence and the topographic component of the interferometric phase, respectively.\n\nIn conclusion, this study combine: PS-InSAR (mm precision), ANN, and RF to model the\n\n\nSolutions\nCurrent policy\nMari Simak Peraturan Larangan Penggunaan Air Tanah di DKI Jakarta\nGroundwater, key to the Sustainable Development Goals (SDGs)\nSDGs goal 6: Clean water and sanitation\n\nIndonesian government set the restriction of groundwater extraction by different fees, will it work?\n\n\n\n\nLimitations and Future\n\nTo validate the estimated LS measurements, it is necessary that the vertical displacement is compared with the GPS measurements.\nAccuracy\nFuture: NISAR (NASA-ISRO SAR EO satellite launch in 2024) with 2 bands (L-band and S-band)"
  },
  {
    "objectID": "week5.html#overview",
    "href": "week5.html#overview",
    "title": "5  Introduction to GEE (Google Earth Engine)",
    "section": "Overview",
    "text": "Overview\nGoogle Earth Engine (GEE) is a cloud-based platform for analyzing and processing geospatial data. It allows users to access and process massive amounts of remote sensing data for various purposes, including land use and land cover analysis, natural resource management, and disaster response. GEE has a powerful processing infrastructure that allows users to apply various geospatial analysis techniques, such as image classification, object detection, and time-series analysis.\nUser Interface (UI) of GEE “Earth Engine Code Editor | Google Earth Engine” (n.d.):\n\n\n\n\nTerms/Jargon Specific to GEE GEE has several terms and jargon that are specific to the platform. For example, ee.Image is a class in the GEE API used to represent an image, ee.ImageCollection is used to represent a collection of images, and ee.Geometry is used to represent geometries like points, lines, and polygons. Other terms include ee.Reducer, ee.Filter, and ee.FeatureCollection. A complete list of GEE-specific terms and jargon can be found in the GEE documentation.\nRelating Spatial Data Formats We Have Seen to GEE GEE can work with various spatial data formats, including GeoTIFF, NetCDF, and SHP. These formats can be uploaded to the GEE platform and processed using GEE’s tools and algorithms. GEE also supports several remote sensing data formats, including Landsat, Sentinel-2, MODIS, and more.\nClient vs. Server-Side GEE processing can occur on both the client-side and server-side. The client-side involves using GEE’s JavaScript API to send requests to the server, visualize data, and process small-scale data. On the other hand, the server-side involves using GEE’s computing infrastructure to process large-scale geospatial data. The server-side can process data more efficiently and effectively than the client-side, making it ideal for large-scale analyses.\nScale (Resolution) GEE works with geospatial data at various scales or resolutions. The platform supports data with a spatial resolution ranging from a few meters to several kilometers. The choice of scale depends on the user’s analysis goals and the spatial and temporal extent of the data.\nProjections GEE supports various map projections, including Geographic, UTM, and Web Mercator. The platform automatically applies the correct projection to the data, ensuring that the data is displayed correctly on the map.\n\n\n\n\nGEE can be accessed through the Google Earth Engine Code Editor, where users can write and run JavaScript code to access and analyze geospatial data. Users can also use GEE’s pre-built tools, such as the Image and Feature Editors, to view and edit geospatial data directly!!\n\nBuilding blocks of data (the data in GEE) The building blocks of data in GEE are images and image collections. Images are raster data with multiple bands, such as satellite imagery, and can be processed using GEE’s built-in functions. Image collections are collections of images, and they can be filtered, sorted, and processed as a group.\nCollections, geometries, and features In GEE, collections are groups of objects, such as a collection of satellite images or a collection of point locations. Geometries are shapes used to define regions of interest, such as a polygon for a city boundary. Features are objects that contain geometries and properties, such as a point location with attributes like temperature and humidity.\nReducing images (e.g., zonal statistics) Reducing images involves computing a statistic or value for each pixel or region in an image. For example, zonal statistics can be used to calculate the mean temperature for each polygon in a feature collection. GEE has built-in functions for reducing images, such as ee.Reducer.mean().\nRegression (over time) Regression can be used to analyze changes in geospatial data over time. For example, linear regression can be used to analyze changes in vegetation over time using satellite imagery. GEE has built-in functions for regression, such as ee.Reducer.linearRegression().\nJoins Joins involve combining data from two or more collections based on a common attribute. For example, a feature collection of cities can be joined with a table of population data based on a common attribute such as city name. GEE has built-in functions for joining data, such as ee.Join.inner().\nMachine learning (intro) GEE also supports machine learning algorithms, such as classification and clustering. Machine learning can be used to classify land cover types using satellite imagery or to identify patterns in geospatial data. GEE has built-in functions for machine learning, such as ee.Classifier.svm()."
  },
  {
    "objectID": "week6.html#overview",
    "href": "week6.html#overview",
    "title": "6  Classification I",
    "section": "Overview",
    "text": "Overview\n\n\n\nImage classification techniques. Source: GIS Geography\n\n\n\n“Image classification is the process of assigning land cover classes to pixels. For example, classes include water, urban, forest, agriculture, and grassland.” - GISGeography (2014)\n\n\n6.0.1 Image Classification in Remote Sensing\nAccording to GISGeography (2014), image classification is a fundamental task in remote sensing and is used for various applications, such as land cover mapping, vegetation analysis, and change detection.\n\n\n6.0.2 Supervised Classification\n\n\n\nSupervised classification. Source: GIS Geography\n\n\n“In supervised classification, you select representative samples for each land cover class. The software then uses these “training sites” and applies them to the entire image. The three basic steps for supervised classification are select training areas, generate signature file, and then classify.” (GISGeography 2014)\nSupervised classification involves using a set of training samples to train a classification algorithm. The training samples are pixels that are known to belong to specific classes, and the classification algorithm assigns each pixel in the image to the class that it is most likely to belong to based on its spectral signature. Maximum likelihood and support vector machine (SVM) are examples of supervised classification algorithms that are commonly used in remote sensing.\n\n\n6.0.3 Unsupervised Classification\n\n\n\nUnsupervised classification. Source: GIS Geography\n\n\nUnsupervised classification, on the other hand, does not require the use of training samples. Instead, the algorithm groups pixels based on their spectral similarity, and the user assigns labels to the resulting clusters. K-means clustering and ISODATA are examples of unsupervised classification algorithms in remote sensing.\n\n\n6.0.4 Object-Based Image Analysis (OBIA)\n\n\n\nOBIA. Source: GIS Geography\n\n\n“Object-based image analysis (OBIA) segments an image by grouping pixels. It doesn’t create single pixels. Instead, it generates objects with different geometries. If you have the right image, objects can be so meaningful that it does the digitizing for you. For example, the segmentation results below highlight buildings. The 2 most common segmentation algorithms are multi-resolution segmentation in eCognition, and the segment mean shift tool in ArcGIS.”(GISGeography 2014)\n\n\n\nOBIA preview. Source: GIS Geography\n\n\nDifferent methods can be used in OBIA to classify the object such as shape, texture, spectral, geographic context, and nearest neighbor.\n\n\n6.0.5 Which One to Use?\nAccording to Blaschke (2010), choosing classification method is highly depends on the spatial resolution of your image. Pixel-based classification (supervised and unsupervised classification) performs well on low spatial resolution image, while OBIA is good for high spatial resolution image."
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "6  Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\n\n\nImage classification techniques. Source: GIS Geography\n\n\n\n“Image classification is the process of assigning land cover classes to pixels. For example, classes include water, urban, forest, agriculture, and grassland.” - GISGeography (2014)\n\n\n\nAccording to GISGeography (2014), image classification is a fundamental task in remote sensing and is used for various applications, such as land cover mapping, vegetation analysis, and change detection.\n\n\n\n\n\n\nSupervised classification. Source: GIS Geography\n\n\n“In supervised classification, you select representative samples for each land cover class. The software then uses these “training sites” and applies them to the entire image. The three basic steps for supervised classification are select training areas, generate signature file, and then classify.” (GISGeography 2014)\nSupervised classification involves using a set of training samples to train a classification algorithm. The training samples are pixels that are known to belong to specific classes, and the classification algorithm assigns each pixel in the image to the class that it is most likely to belong to based on its spectral signature. Maximum likelihood and support vector machine (SVM) are examples of supervised classification algorithms that are commonly used in remote sensing.\n\n\n\n\n\n\nUnsupervised classification. Source: GIS Geography\n\n\nUnsupervised classification, on the other hand, does not require the use of training samples. Instead, the algorithm groups pixels based on their spectral similarity, and the user assigns labels to the resulting clusters. K-means clustering and ISODATA are examples of unsupervised classification algorithms in remote sensing.\n\n\n\n\n\n\nOBIA. Source: GIS Geography\n\n\n“Object-based image analysis (OBIA) segments an image by grouping pixels. It doesn’t create single pixels. Instead, it generates objects with different geometries. If you have the right image, objects can be so meaningful that it does the digitizing for you. For example, the segmentation results below highlight buildings. The 2 most common segmentation algorithms are multi-resolution segmentation in eCognition, and the segment mean shift tool in ArcGIS.”(GISGeography 2014)\n\n\n\nOBIA preview. Source: GIS Geography\n\n\nDifferent methods can be used in OBIA to classify the object such as shape, texture, spectral, geographic context, and nearest neighbor.\n\n\n\nAccording to Blaschke (2010), choosing classification method is highly depends on the spatial resolution of your image. Pixel-based classification (supervised and unsupervised classification) performs well on low spatial resolution image, while OBIA is good for high spatial resolution image.\n\nImage classification in remote sensing involves identifying and labeling different land cover and land use types within an image using machine learning algorithms. Classified data can be used for various applications, such as land cover mapping, vegetation analysis, and change detection.\nTo classify remotely sensed data, there are several machine learning algorithms that can be used. Classification and regression trees (CART), decision trees, and regression trees are examples of decision tree-based algorithms that are commonly used in remote sensing. However, overfitting can be a problem with decision tree-based algorithms, which can be mitigated using techniques such as random forest."
  },
  {
    "objectID": "week6.html#applications",
    "href": "week6.html#applications",
    "title": "6  Classification I",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nA study by Mehmood et al. (2022) entitled: Remote Sensing Image Classification: A Comprehensive Review and Applications, reviewed remote sensing image classification techniques and their applications.\nThe key takeaways of the study include:\n\nRemote sensing image classification is an essential tool for mapping land cover and land use, identifying features and objects on the Earth’s surface, and monitoring changes over time.\nA variety of techniques have been developed for remote sensing image classification, including supervised and unsupervised classification, object-based classification, and deep learning-based classification.\nThe accuracy of remote sensing image classification is affected by a number of factors, including sensor characteristics, image preprocessing, feature extraction, classification algorithm, and ground truth data.\nApplications of remote sensing image classification include environmental monitoring, agriculture, urban planning, disaster management, and defense and security.\nThe study analyzes various datasets used in remote sensing image classification, including multispectral and hyperspectral imagery, synthetic aperture radar (SAR), and LiDAR data. The methods used in the study involve a systematic review of research articles and a meta-analysis of the results.\n\n\n\n\nBlock diagram representing details about spectral feature extraction by Mehmood et al. (2022)\n\n\nThe results of the study indicate that deep learning-based classification (e.g. CNN) methods are becoming increasingly popular due to their ability to learn complex patterns in the data. However, traditional classification methods such as maximum likelihood and support vector machines are still widely used and can provide good results when applied appropriately.\nWhat’s interesting is that this study also review the combination of two techniques: pixel-based multilayer perceptron and CNN. The dataset contains images of both urban and rural lands of different land uses of Southampton. It said that the proposed method outperforms the existing deep learning methods. The accuracies achieved from this proposed model are 90.93% for urban and 89.64% for rural lands.\n\n\n\nBlock diagram representing details about spectral feature extraction by Mehmood et al. (2022)\n\n\nFull review by Mehmood et al. (2022):"
  },
  {
    "objectID": "week6.html#personal-reflection",
    "href": "week6.html#personal-reflection",
    "title": "6  Classification I",
    "section": "6.3 Personal Reflection",
    "text": "6.3 Personal Reflection\nHonestly, I am a bit overwhelmed by the lecture material this time because we have to learn some (I mean many) of the new terms in image classification in remote sensing. Also, a bit tricky to understand which classification we can use on our data. However, I enjoy learn something new, and I think the slides and practical cover a wide range of topics and provide detailed explanations of the different techniques and algorithms used. In addition, the resources provided are useful for student to explore!\n\n\n\n\nBlaschke, T. 2010. “Object Based Image Analysis for Remote Sensing.” ISPRS Journal of Photogrammetry and Remote Sensing 65 (1): 2–16. https://doi.org/10.1016/j.isprsjprs.2009.06.004.\n\n\nGISGeography. 2014. “Image Classification Techniques in Remote Sensing.” GIS Geography. May 2, 2014. https://gisgeography.com/image-classification-techniques-remote-sensing/.\n\n\nMehmood, Maryam, Ahsan Shahzad, Bushra Zafar, Amsa Shabbir, and Nouman Ali. 2022. “Remote Sensing Image Classification: A Comprehensive Review and Applications.” Mathematical Problems in Engineering 2022 (August): e5880959. https://doi.org/10.1155/2022/5880959."
  },
  {
    "objectID": "week7.html#overview",
    "href": "week7.html#overview",
    "title": "7  Classification II",
    "section": "Overview",
    "text": "Overview\n\n7.0.1 More into Segmentation and Object-based analysis\n“A lot of people want to do object-based classification because of an improvement of signal noise” - Noel Gorelick\nI am interested in this talk by Noel Gorelick about image segmentation and object based methods.\n\nObject-based image analysis is a two-step approach that involves segmenting the image using an algorithm and then classifying the landscape objects using either supervised or unsupervised approaches. The SNIC segmentation algorithm is discussed as an example of an object-based image analysis approach available in Earth Engine, which has been used in various research applications, including mapping land use, wetlands, burned areas, sustainable development goal indicators, and ecosystem services.\n\n\n7.0.2 Accuracy Assessment\nThe research paper titled “Accuracy Dimensions in Remote Sensing” by Barsi et al. (2018) focuses on the various accuracy dimensions that need to be considered when performing remote sensing applications. The study discusses the different accuracy metrics and how they can be used to evaluate the quality of remote sensing products.\n\n\nThe key takeaways from this paper are:\n\nThere are multiple dimensions to consider when assessing the accuracy of remote sensing products, including spatial accuracy, spectral accuracy, temporal accuracy, and geometric precision. Below is the example of accrucary in land cover application.\n\n\n\n“The confusion matrix, also known as error matrix is a specific table that allows visualization of the performance of a classification. The matrix is an excellent base to derive further quality measures. Two basic types exist: the binary and the multiclass confusion matrix.”(@ Barsi et al. 2018)\n\n\n\nBarsi et al. (2018) provide a useful overview of the different accuracy dimensions in remote sensing and how they can be evaluated using different accuracy metrics. The study provides insights into the factors that affect the accuracy of remote sensing products and highlights the importance of selecting an appropriate accuracy metric for a specific application."
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "7  Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\n\n“A lot of people want to do object-based classification because of an improvement of signal noise” - Noel Gorelick\nI am interested in this talk by Noel Gorelick about image segmentation and object based methods.\n\nObject-based image analysis is a two-step approach that involves segmenting the image using an algorithm and then classifying the landscape objects using either supervised or unsupervised approaches. The SNIC segmentation algorithm is discussed as an example of an object-based image analysis approach available in Earth Engine, which has been used in various research applications, including mapping land use, wetlands, burned areas, sustainable development goal indicators, and ecosystem services.\n\n\n\nThe research paper titled “Accuracy Dimensions in Remote Sensing” by Barsi et al. (2018) focuses on the various accuracy dimensions that need to be considered when performing remote sensing applications. The study discusses the different accuracy metrics and how they can be used to evaluate the quality of remote sensing products.\n\n\nThe key takeaways from this paper are:\n\nThere are multiple dimensions to consider when assessing the accuracy of remote sensing products, including spatial accuracy, spectral accuracy, temporal accuracy, and geometric precision. Below is the example of accrucary in land cover application.\n\n\n\n“The confusion matrix, also known as error matrix is a specific table that allows visualization of the performance of a classification. The matrix is an excellent base to derive further quality measures. Two basic types exist: the binary and the multiclass confusion matrix.”(@ Barsi et al. 2018)\n\n\n\nBarsi et al. (2018) provide a useful overview of the different accuracy dimensions in remote sensing and how they can be evaluated using different accuracy metrics. The study provides insights into the factors that affect the accuracy of remote sensing products and highlights the importance of selecting an appropriate accuracy metric for a specific application.\n\nUtilisation of segmentation and object-based image classification for urban and built environment turns out very useful and become easier. Thanks to the more high spatial resolution of satellite image today. By segmenting an image into different objects such as buildings, roads, and green spaces, and then classifying those objects based on their characteristics, we can create detailed maps of urban areas that can be used for city planning and management. However, to assess accuracy, we have to be careful on various parameters that already mentioned above."
  },
  {
    "objectID": "week7.html#applications",
    "href": "week7.html#applications",
    "title": "7  Classification II",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nInterestingly, from the other module (CASA0006 Data Science for Spatial System), we learn baout object detection in satellite imagery using YOLOv5.\nThis application combining the YOLOv5 deep learning model to detect objects in satellite imagery. Steps in this application process:\n\nObject detection in satellite imagery\nTraining a deep learning model on a custom dataset\nDynamic inference using Google Earth Engine\n\nYOLOv5 is a convolutional neural networks (CNNs) to identify patterns in images. YOLOv5 divides images into a grid and predicts the location and size of objects within each cell. The model is trained on a dataset containing 80 different objects and can be retrained on labeled satellite imagery datasets for object detection in satellite images from Google Earth Engine.\n\n\n\nYOLOv5. Source: Ollie Ballinger\n\n\nAfter got the trained model, it is used to conduct object detection. This apPlications accessing GEE from python notebook.\n\n\n\nObject Detection. Source: Ollie Ballinger\n\n\nAs we can see in the figure, the result is quite promising with high degree of accuracy."
  },
  {
    "objectID": "week7.html#personal-reflection",
    "href": "week7.html#personal-reflection",
    "title": "7  Classification II",
    "section": "7.3 Personal Reflection",
    "text": "7.3 Personal Reflection\nI found this week’s topics to be both challenging and rewarding. The concepts of segmentation and object-based classification can be complex, but they are essential for many remote sensing applications. And most importantly, it makes quantifying parameter easier in the country that has not develop a good data base or open data, such as Indonesia. I look forward to utilise object detection more to monitor the unrevealed data such as illegal mining.\n\n\n\n\nBarsi, Á., Zs. Kugler, I. László, Gy. Szabó, and H. M. Abdulmutalib. 2018. “ACCURACY DIMENSIONS IN REMOTE SENSING.” The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLII-3 (April): 61–67. https://doi.org/10.5194/isprs-archives-XLII-3-61-2018."
  },
  {
    "objectID": "week8.html#overview",
    "href": "week8.html#overview",
    "title": "8  Temperature and Policy",
    "section": "Overview",
    "text": "Overview\n\n8.0.1 What is Urban Heat Island (UHI)?\nSo, it is about temperature raise because of the built environment. The built environment such as roads, buildings, pavements and other unnatural surfaces absorb and retain heat. That is why city is hotter than surrounding rural areas.\n\n\n\nUrban Heat Island (UHI). Source: Public Health Notes\n\n\n\n\n\nUrban Heat Island (UHI). Source: US EPA\n\n\nBy looking at this figure, I just found out that the surface temperature is so much higher than the air temperature during day time. Not only that, it is also vary more than the atmospheric temperature. However, interestingly, it has similar value and not much vary on the night time."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "\n8  Temperature and Policy\n",
    "section": "\n8.1 Summary",
    "text": "8.1 Summary\n\nSo, it is about temperature raise because of the built environment. The built environment such as roads, buildings, pavements and other unnatural surfaces absorb and retain heat. That is why city is hotter than surrounding rural areas.\n\n\nUrban Heat Island (UHI). Source: Public Health Notes\n\n\n\n\nUrban Heat Island (UHI). Source: US EPA\n\n\nBy looking at this figure, I just found out that the surface temperature is so much higher than the air temperature during day time. Not only that, it is also vary more than the atmospheric temperature. However, interestingly, it has similar value and not much vary on the night time. But still, downtown areas experience higher temperature than surrounding rural areas.\n\nSome of the factors resulting on UHI according to US EPA (2014):\n\nReduced natural landscpae in urban areas The more we cover our natural landscape (vegetation, water) with concrete materials, the higher temperature we will get because hard, dry surfaces provide less shade and moisture than natural landscapes.\nUrban material properties Heat island often build throughout the day and become more pronounced in the night time due to the slow release of heat from urban materials (e.g. concrete, glass)\nUrban geometry The dimensions of the buildings are also can promote UHI. The higher buildings in the same blocks, the more it will influence the wind flow and block it.\nHeat generated from human activities Industrial activities, using ACs, and transportations also contribute to the emission and will heat the environment.\nWeather and geography Calm and clear weather conditions result in more severe heat islands by maximizing the amount of solar energy reaching urban surfaces and minimizing the amount of heat that can be carried away. Conversely, strong winds and cloud cover suppress heat island formation. Geographic features can also impact the heat island effect. For example, nearby mountains can block wind from reaching a city, or create wind patterns that pass through a city.\n\nSeveral global policies about UHI:\n\nUN Habitat New Urban Agenda: Standards and principles for planning, construction, development, management and urban improvement Point 54: reducing inefficient mmobility, congestion, and air pollution. Point 79: climate change adaptation and mitigation Point 37: inclusive and accessible green spaces.\nUN SDGs 11: Make cities and human settlements inclusive, safe, resilient and sustainable Investing in parks and green spaces in urban areas will help to ameliorate the urban heat island effect and improve air quality in urban spaces. We work with national coordination units that support integrated urban planning and mapping and promote sustainable heating and cooling in related and cross-sector policy frameworks at multiple levels\nCOP26: Beating the Heat: A Sustainable Cooling Handbook for Cities\n\nSuperblocks are super interesting! (One of the reason I want to go to Barcelona!)\n\n\nSuperblocks Model. Source: regenerativedesign.world\n\n\n\n\nSource: regenerativedesign.world\n\n\nMy thought on this is that it is closed to 15 minutes city model. However, can it be applied to other cities?"
  },
  {
    "objectID": "week8.html#applications",
    "href": "week8.html#applications",
    "title": "\n8  Temperature and Policy\n",
    "section": "\n8.2 Applications",
    "text": "8.2 Applications\nOne of the applications of this environmental-related issues and problem solving using remote sensing is presented in our group presentation Landfill Monitoring Dashboard by PoliShe\nWe are providing surface temperature monitoring as one of the product using study by Hanami et al. (2022), which related to this week’s lecture with study case in Makassar,South Sulawesi, Indonesia.\nPolicy that we aim to focus is the Landfill Monitoring policy which in-line with the Green House Gas Emission National Policy the New 2030 Climate Targets"
  },
  {
    "objectID": "week8.html#personal-reflection",
    "href": "week8.html#personal-reflection",
    "title": "\n8  Temperature and Policy\n",
    "section": "\n8.3 Personal Reflection",
    "text": "8.3 Personal Reflection\nThis week we’re talking mostly about urban heat island and policies that are related to environmental issues in cities around the world. The lecturer said some of the policies are made unclear and no specific approach. However, I must argue this point of view because as a person who grew and lives in a developing countries, I have to say specific rules/policy are not relatable to all countries (especially developing country).\nAs the lecturer mentioned, the COP26 Beat the Heat Handbook only give a list of general parameters and have 200+ pages to read. I think it is because the organization gives the national authority, for example Indonesian government, a flexibility to approach their solution based on the situation, condition, and data in Indonesia. The next stage of the project is the one that state a clear goal and approach while corresponding with national government (e.g. Green House Gas Emission reduction project by monitoring landfill).\nOverall, I agree that we cannot just make a policy because of the sake of it. Data-driven approach is the most factual and practical approach to solve real-world problem and make recommendations to the policy makers. The establishment of public policy must include the research expert study and recommendations so that we don’t waste money doing something that are pointless (and this still happens a lot  ).\n\n\n\n\nHanami, Z. A., A. D. Damayanti, T. Takeda, and H. Alimuddin. 2022. “Estimation of Makassar’s Landfill Surface Temperature and Its Surroundings Using Remote Sensing.” IOP Conference Series: Earth and Environmental Science 1117 (1): 012055. https://doi.org/10.1088/1755-1315/1117/1/012055.\n\n\nUS EPA, OAR. 2014. “Learn About Heat Islands.” Overviews and Factsheets. June 17, 2014. https://www.epa.gov/heatislands/learn-about-heat-islands."
  }
]